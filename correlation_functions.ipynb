{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as fits\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import treecorr\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only necessary columns from the galaxy shape catalogue\n",
    "shape_cat_path = '/n17data/mkilbing/astro/data/CFIS/v1.x/ShapePipe/v1.5.x/v1.5.3/unions_shapepipe_cut_struc_2024_v1.5.3.fits'\n",
    "shapes = fits.open(shape_cat_path)[1].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desi_lrg_ngc = fits.open('/home/murray/desi_data/LRG_NGC_clustering.dat.fits')[1].data\n",
    "desi_lrg_sgc = fits.open('/home/murray/desi_data/LRG_SGC_clustering.dat.fits')[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( desi_lrg_ngc ) , len( desi_lrg_sgc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DESI north and south galaxy catalogues\n",
    "galaxies = np.concatenate((desi_lrg_ngc, desi_lrg_sgc))\n",
    "\n",
    "# Check the length of the combined catalogue\n",
    "len(galaxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "randoms_ngc = fits.open('/n17data/murray/desi_data/LRG_NGC_0_clustering.ran.fits')[1].data\n",
    "randoms_sgc = fits.open('/n17data/murray/desi_data/LRG_SGC_0_clustering.ran.fits')[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DESI north and south galaxy catalogues\n",
    "randoms = np.concatenate((randoms_ngc, randoms_sgc))\n",
    "\n",
    "# Check the length of the combined catalogue\n",
    "len(randoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SkyCoord objects for matching\n",
    "galaxy_coords = coord.SkyCoord(ra=galaxies['RA']*u.degree, dec=galaxies['DEC']*u.degree)\n",
    "shape_coords = coord.SkyCoord(ra=shapes['RA']*u.degree, dec=shapes['Dec']*u.degree)\n",
    "\n",
    "# Perform the matching\n",
    "idx, d2d, d3d = galaxy_coords.match_to_catalog_sky(shape_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a maximum separation criterion (e.g., 1 arcsecond)\n",
    "max_sep = 1 * u.arcsec\n",
    "matches = d2d < max_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_max = 50\n",
    "theta_bins = np.linspace( 0 , temp_max , 100 )\n",
    "theta_cents = ( theta_bins[1:] + theta_bins[:-1] )/2.\n",
    "counts, _ = np.histogram( d2d.arcsec[ d2d.arcsec < temp_max ] , bins = theta_bins )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.plot( theta_cents , counts )\n",
    "plt.xlabel('Separation (arcseconds)')\n",
    "plt.ylabel('Number of Matches')\n",
    "plt.title('Histogram of Separations Between Catalogues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( galaxies['RA'][matches] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the matched catalogues\n",
    "matched_galaxy_ra = galaxies['RA'][matches]\n",
    "matched_galaxy_dec = galaxies['DEC'][matches]\n",
    "matched_galaxy_redshift = galaxies['Z'][matches]\n",
    "matched_shape_ra = shapes['RA'][idx[matches]]\n",
    "matched_shape_dec = shapes['Dec'][idx[matches]]\n",
    "matched_shape_e1 = shapes['e1'][idx[matches]]\n",
    "matched_shape_e2 = shapes['e2'][idx[matches]]\n",
    "matched_shape_w = shapes['w_iv'][idx[matches]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new FITS table with the matched columns\n",
    "cols = [\n",
    "    fits.Column(name='RA', format='E', array=matched_galaxy_ra),\n",
    "    fits.Column(name='Dec', format='E', array=matched_galaxy_dec),\n",
    "    fits.Column(name='e1', format='E', array=matched_shape_e1),\n",
    "    fits.Column(name='e2', format='E', array=matched_shape_e2),\n",
    "    fits.Column(name='w_iv', format='E', array=matched_shape_w),\n",
    "    fits.Column(name='galaxy_RA', format='E', array=matched_galaxy_ra),\n",
    "    fits.Column(name='galaxy_Dec', format='E', array=matched_galaxy_dec),\n",
    "    fits.Column(name='redshift', format='E', array=matched_galaxy_redshift)\n",
    "]\n",
    "\n",
    "hdu = fits.BinTableHDU.from_columns(cols)\n",
    "hdu.writeto('unions_desi_lrg_catalogue.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_position_catalogue = fits.open('unions_desi_lrg_catalogue.fits')[1].data\n",
    "len( shape_position_catalogue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_position_catalogue = fits.open('unions_desi_elg_catalogue.fits')[1].data\n",
    "len( shape_position_catalogue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_position_catalogue = fits.open('unions_desi_bgs_catalogue.fits')[1].data\n",
    "len( shape_position_catalogue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_position_catalogue = fits.open('unions_desi_lrg+elg_catalogue.fits')[1].data\n",
    "len( shape_position_catalogue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import FlatLambdaCDM\n",
    "cosmo = FlatLambdaCDM( 70 , 0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = 'Ra'\n",
    "dec = 'DEC'\n",
    "redshift = 'redshift'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove random shapes and positions outside of the survey footprints\n",
    "# Convert RA, Dec to HEALPix pixel indices\n",
    "nside = 128\n",
    "pixels_positions = hp.ang2pix(nside, galaxies['RA'], galaxies['DEC'] , lonlat=True)\n",
    "pixels_shapes = hp.ang2pix(nside,  shape_position_catalogue[ra], shape_position_catalogue[dec] , lonlat=True)\n",
    "#pixels_unions_shapes = hp.ang2pix(nside,  shapes['Ra'], shapes['Dec'] , lonlat=True)\n",
    "pixels_random_positions = hp.ang2pix(nside,  randoms['RA'], randoms['DEC'] , lonlat=True)\n",
    "\n",
    "# Create HEALPix count maps\n",
    "position_map = np.bincount(pixels_positions, minlength=hp.nside2npix(nside))\n",
    "shape_map = np.bincount(pixels_shapes, minlength=hp.nside2npix(nside))\n",
    "#unions_shape_map = np.bincount(pixels_unions_shapes, minlength=hp.nside2npix(nside))\n",
    "random_position_map = np.bincount(pixels_random_positions, minlength=hp.nside2npix(nside))\n",
    "\n",
    "# Create a mask: pixels with at least one object\n",
    "position_mask = (position_map > 0) \n",
    "shape_mask = (shape_map>0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview( position_map , coord ='E' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview( shape_mask , coord ='E' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp.mollview( unions_shape_map, coord ='E' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview( random_position_map , coord ='E' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist( galaxies['Z'] ,100,histtype='step',density=True,label='positions')\n",
    "plt.hist( randoms['Z'] ,100,histtype='step',density=True,label='eboss randoms')\n",
    "plt.hist( shape_position_catalogue['redshift'],100,histtype='step',density=True,label='shapes')\n",
    "# plt.hist(random_positions[redshift],100,histtype='step',density=True,label='Random positions',lw=2,alpha=0.5,color='C0')\n",
    "# plt.hist(random_shapes[redshift],100,histtype='step',density=True,label='Random shapes',lw=2,alpha=0.5,color='C1')\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('n')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(np.sin(np.deg2rad(galaxies['DEC'])),100,histtype='step',density=True,label='positions')\n",
    "#plt.hist(np.sin(np.deg2rad(shape_position_catalogue['Dec'])),100,histtype='step',density=True,label='shapes')\n",
    "plt.hist(np.sin(np.deg2rad(randoms['DEC'])),100,histtype='step',density=True,label='Random positions',lw=2,alpha=0.5,color='C0')\n",
    "plt.xlabel('sin(dec)')\n",
    "plt.ylabel('n')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(galaxies['RA'],100,histtype='step',density=True,label='positions')\n",
    "#plt.hist(shape_position_catalogue['RA'],100,histtype='step',density=True,label='shapes')\n",
    "plt.hist(randoms['RA'],100,histtype='step',density=True,label='Random positions',lw=2,alpha=0.5,color='C0')\n",
    "plt.xlabel('ra')\n",
    "plt.ylabel('n')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only 1D columns from the galaxy sample\n",
    "columns_to_use = ['RA', 'DEC', 'Z',  'WEIGHT']\n",
    "positions = pd.DataFrame({col: galaxies[col] for col in columns_to_use})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "shapes = pd.DataFrame.from_records( shape_position_catalogue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_positions = pd.DataFrame.from_records( randoms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions.shape, shapes.shape, random_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[redshift] = positions['Z']\n",
    "positions['w'] = np.ones( len( positions ))\n",
    "shapes['w'] = shapes['w_iv']\n",
    "shapes['DEC'] = shapes['Dec']\n",
    "random_positions[redshift] = random_positions['Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_positions = random_positions.iloc[::5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( randoms ) / len( random_positions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'd'\n",
    "\n",
    "positions.loc[:,d] = cosmo.comoving_distance(positions[redshift]).value\n",
    "shapes.loc[:,d]    = cosmo.comoving_distance(shapes[redshift]).value\n",
    "\n",
    "random_positions[d] = cosmo.comoving_distance(random_positions[redshift]).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min( positions['d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions['x'] = positions[d]*np.cos(np.deg2rad(positions['DEC']))*np.cos(np.deg2rad(positions['RA']))\n",
    "positions['y'] = positions[d]*np.cos(np.deg2rad(positions['DEC']))*np.sin(np.deg2rad(positions['RA']))\n",
    "positions['z'] = positions[d]*np.sin(np.deg2rad(positions['DEC']))\n",
    "\n",
    "shapes['x'] = shapes[d]*np.cos(np.deg2rad(shapes['DEC']))*np.cos(np.deg2rad(shapes['RA']))\n",
    "shapes['y'] = shapes[d]*np.cos(np.deg2rad(shapes['DEC']))*np.sin(np.deg2rad(shapes['RA']))\n",
    "shapes['z'] = shapes[d]*np.sin(np.deg2rad(shapes['DEC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes['e1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nans = shapes['e1'].isna().sum()\n",
    "print(f\"Number of NaNs: {num_nans}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nans = shapes['e2'].isna().sum()\n",
    "print(f\"Number of NaNs: {num_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nans = shapes['w'].isna().sum()\n",
    "print(f\"Number of NaNs: {num_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_catalogue = treecorr.Catalog( x=positions['x'], \n",
    "                                       y=positions['y'], \n",
    "                                       z=positions['z'],\n",
    "                                       w=positions['w'])\n",
    "shape_catalogue = treecorr.Catalog( x=shapes['x'], \n",
    "                                    y=shapes['y'], \n",
    "                                    z=shapes['z'], \n",
    "                                    g1 = shapes['e1'],\n",
    "                                    g2 = shapes['e2'], \n",
    "                                    w=shapes['w'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.sqrt( shapes['e1']**2 + shapes['e2']**2 )\n",
    "\n",
    "stick_catalogue = treecorr.Catalog( x=shapes['x'], \n",
    "                                    y=shapes['y'], \n",
    "                                    z=shapes['z'], \n",
    "                                    g1 = shapes['e1'] / e ,\n",
    "                                    g2 = shapes['e2'] / e , \n",
    "                                    w= np.ones( shapes['w'].shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_positions['x'] = random_positions[d]*np.cos(np.deg2rad(random_positions['DEC']))*np.cos(np.deg2rad(random_positions['RA']))\n",
    "random_positions['y'] = random_positions[d]*np.cos(np.deg2rad(random_positions['DEC']))*np.sin(np.deg2rad(random_positions['RA']))\n",
    "random_positions['z'] = random_positions[d]*np.sin(np.deg2rad(random_positions['DEC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_position_catalogue = treecorr.Catalog( x=random_positions['x'], \n",
    "                                              y=random_positions['y'], \n",
    "                                              z=random_positions['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single rpar bin\n",
    "def process_rpar_bin(min_rpar, max_rpar , min_sep, max_sep, nbins , bin_slop  ):\n",
    "    print('Running between rpar =', min_rpar, 'and rpar =', max_rpar)\n",
    "\n",
    "    # Create the NNCorrelation objects\n",
    "    nn = treecorr.NNCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar, bin_type='Linear' , bin_slop = bin_slop ) \n",
    "    rr = treecorr.NNCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar, bin_type='Linear' , bin_slop = bin_slop ) \n",
    "    dr = treecorr.NNCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar, bin_type='Linear' , bin_slop = bin_slop )\n",
    "\n",
    "    # Process the position and random catalogues\n",
    "    nn.process(position_catalogue, position_catalogue, metric='Rperp')\n",
    "    rr.process(random_position_catalogue, random_position_catalogue, metric='Rperp')\n",
    "    dr.process(position_catalogue, random_position_catalogue, metric='Rperp')\n",
    "\n",
    "    # Calculate the Landy-Szalay estimator\n",
    "    xi, varxi = nn.calculateXi(rr=rr, dr=dr)\n",
    "    r = np.exp(nn.meanlogr)\n",
    "    sig = np.sqrt(varxi)\n",
    "\n",
    "    return xi, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range and number of bins for rpar\n",
    "rpar_bins = np.linspace(-200, 200, 41 )  \n",
    "\n",
    "# Define the parameters for the correlation function\n",
    "min_sep = 0.5  # Minimum separation in Mpc\n",
    "max_sep = 200  # Maximum separation in Mpc\n",
    "nbins = 40     # Number of bins\n",
    "bin_slop = 0.1 # Bin slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "xi_results = []\n",
    "r_results = []\n",
    "\n",
    "# Iterate over rpar bins\n",
    "for i in range(len(rpar_bins) - 1):\n",
    "\n",
    "    min_rpar = rpar_bins[i]\n",
    "    max_rpar = rpar_bins[i + 1]\n",
    "\n",
    "    xi , r = process_rpar_bin( min_rpar, max_rpar , min_sep , max_sep , nbins , bin_slop )\n",
    "\n",
    "    # Store the results\n",
    "    xi_results.append(xi)\n",
    "    r_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_results = np.array(xi_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( np.log10( np.abs( xi_results ) ) )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "sigma = 1\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "smoothed_xi_results = gaussian_filter(xi_results, sigma= sigma )\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow( np.log10( smoothed_xi_results ), extent=[0, 200, -200, 200])\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming r_perp_bins and r_par_bins are defined\n",
    "r_perp_cents = r\n",
    "r_par_cents = ( rpar_bins[1:] + rpar_bins[:-1] )/2.\n",
    "\n",
    "# Flatten the xi_results and r_results arrays\n",
    "xi_flat = np.array(xi_results).flatten()\n",
    "r_perp_flat = np.array([r_perp_cents] * len(r_par_cents)).flatten()\n",
    "r_par_flat = np.array([[r_par] * len(r_perp_cents) for r_par in r_par_cents]).flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_par_flat.shape, r_perp_flat.shape, xi_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate s for each pair of (r_perp, r_par)\n",
    "s = np.sqrt(r_perp_flat**2 + r_par_flat**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min( s ) , np.max( s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate s for each pair of (r_perp, r_par)\n",
    "s = np.sqrt(r_perp_flat**2 + r_par_flat**2)\n",
    "\n",
    "# Define bins for s\n",
    "s_bins = np.linspace( 50 , 250 , 40)\n",
    "\n",
    "# Initialize arrays to store the results\n",
    "xi_s = np.zeros(len(s_bins) - 1)\n",
    "var_xi_s = np.zeros(len(s_bins) - 1)\n",
    "counts = np.zeros(len(s_bins) - 1)\n",
    "\n",
    "# Bin the results by s and calculate the radial averages and variances\n",
    "for i in range(len(s_bins) - 1):\n",
    "    mask = (s >= s_bins[i]) & (s < s_bins[i + 1])\n",
    "    if np.any(mask):  # Check if there are any elements in the mask\n",
    "        xi_s[i] = np.mean(xi_flat[mask])\n",
    "        var_xi_s[i] = np.var(xi_flat[mask])\n",
    "        counts[i] = np.sum(mask)\n",
    "\n",
    "# Handle cases where there are no counts in a bin\n",
    "xi_s[counts == 0] = np.nan\n",
    "var_xi_s[counts == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cents = ( s_bins[1:] + s_bins[:-1] )/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar( s_cents , xi_s * s_cents**2, fmt='o' , yerr = var_xi_s**0.5  * s_cents**2 / counts**0.5 )\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Xi(s)')\n",
    "plt.title('Radial Averages of Xi(s)')\n",
    "plt.xlim( 10 , 200 )\n",
    "plt.ylim( -0.01 , 0.03 * 100**2 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar( s_cents , xi_s , fmt='o' , yerr = var_xi_s**0.5 / counts**0.5 )\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Xi(s)')\n",
    "plt.title('Radial Averages of Xi(s)')\n",
    "plt.xlim( 50 , 200 )\n",
    "#plt.ylim( -0.01 , 0.03  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.coolwarm(np.linspace(0, 1, len(xi_results)))\n",
    "\n",
    "for xi, color in zip(xi_results, colors):\n",
    "    plt.plot(r, xi, color=color)\n",
    "    plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shear stuff\n",
    "# Function to process a single rpar bin\n",
    "def process_ng_rpar_bin( shape_catalogue , min_rpar, max_rpar , min_sep, max_sep , nbins ):\n",
    "    print('Running between rpar =', min_rpar, 'and rpar =', max_rpar)\n",
    "\n",
    "    # Create the NNCorrelation objects\n",
    "    ng = treecorr.NGCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar, bin_type='Linear') \n",
    "    rg = treecorr.NGCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar, bin_type='Linear') \n",
    "\n",
    "    # Process the position and random catalogues\n",
    "    ng.process(position_catalogue, shape_catalogue , metric='Rperp')\n",
    "    rg.process(random_position_catalogue, shape_catalogue, metric='Rperp')\n",
    "\n",
    "    # Calculate the Landy-Szalay estimator\n",
    "    xi_p , xi_x , _ = ng.calculateXi( rg = rg )\n",
    "    r = np.exp(ng.meanlogr)\n",
    "\n",
    "    return r , xi_p , xi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range and number of bins for rpar\n",
    "rpar_bins = np.linspace(-200, 200, 201 )  \n",
    "\n",
    "# Define the parameters for the correlation function\n",
    "min_sep = 0.5  # Minimum separation in Mpc\n",
    "max_sep = 200  # Maximum separation in Mpc\n",
    "nbins = 100     # Number of bins\n",
    "bin_slop = 0.1 # Bin slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r , w_xi_p , w_xi_x = process_ng_rpar_bin( shape_catalogue , -30, 30 , min_sep , max_sep , nbins )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( r  * h, w_xi_p )\n",
    "plt.plot( r  * h, w_xi_x )\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$\\xi_{g+}(s)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "xi_gn_p_results = []\n",
    "xi_gn_x_results = []\n",
    "r_results = []\n",
    "\n",
    "# Iterate over rpar bins\n",
    "for i in range(len(rpar_bins) - 1):\n",
    "\n",
    "    min_rpar = rpar_bins[i]\n",
    "    max_rpar = rpar_bins[i + 1]\n",
    "\n",
    "    r , xi_p , xi_x = process_ng_rpar_bin( shape_catalogue , min_rpar, max_rpar , min_sep , max_sep , nbins )\n",
    "\n",
    "    # Store the results\n",
    "    xi_gn_p_results.append( xi_p )\n",
    "    xi_gn_x_results.append( xi_x )\n",
    "    r_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_gn_p_results = np.array(xi_gn_p_results)\n",
    "xi_gn_x_results = np.array(xi_gn_x_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10 , 5 ))\n",
    "plt.subplot(121)\n",
    "plt.imshow( np.log10( np.abs( xi_gn_p_results ) ) , extent = [ 0 , 200 , -200 , 200 ] )\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow( np.log10( np.abs( xi_gn_x_results ) ) , extent = [ 0 , 200 , -200 , 200 ] )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(xi_gn_p_results, extent=[0, 200, -200, 200])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(xi_gn_x_results, extent=[0, 200, -200, 200])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('lrg_xi_gn_p_results',xi_gn_p_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('lrg_xi_gn_x_results',xi_gn_x_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_gn_p_results = np.load('lrg_xi_gn_p_results.npy')\n",
    "xi_gn_x_results = np.load('lrg_xi_gn_x_results.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "sigma = 1.5\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "smoothed_xi_gn_p_results = gaussian_filter(xi_gn_p_results, sigma= sigma )\n",
    "smoothed_xi_gn_x_results = gaussian_filter(xi_gn_x_results, sigma= sigma )\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(smoothed_xi_gn_p_results, extent=[0, 200, -200, 200])\n",
    "plt.xlim(  0 , 60 )\n",
    "plt.ylim( -60 , 60 )\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(smoothed_xi_gn_x_results, extent=[0, 200, -200, 200])\n",
    "plt.xlim(  0 , 60 )\n",
    "plt.ylim( -60 , 60 )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.5\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "smoothed_xi_gn_p_results = gaussian_filter(xi_gn_p_results, sigma= sigma )\n",
    "smoothed_xi_gn_x_results = gaussian_filter(xi_gn_x_results, sigma= sigma )\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.log(-smoothed_xi_gn_p_results ), extent=[0, 200, -200, 200])\n",
    "plt.xlim(  0 , 60 )\n",
    "plt.ylim( -60 , 60 )\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.log(-smoothed_xi_gn_x_results ), extent=[0, 200, -200, 200])\n",
    "plt.xlim(  0 , 60 )\n",
    "plt.ylim( -60 , 60 )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 2.5\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "smoothed_xi_gn_p_results = gaussian_filter(xi_gn_p_results, sigma= sigma )\n",
    "smoothed_xi_gn_x_results = gaussian_filter(xi_gn_x_results, sigma= sigma )\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(smoothed_xi_gn_p_results, extent=[0, 200, -200, 200] , vmin =-1e-3, vmax=1e-3 )\n",
    "plt.xlim(  0 , 60 )\n",
    "plt.ylim( -60 , 60 )\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(smoothed_xi_gn_x_results, extent=[0, 200, -200, 200] , vmin =-1e-3, vmax=1e-3 )\n",
    "plt.xlim(  0 , 60 )\n",
    "plt.ylim( -60 , 60 )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "sigma = 2\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "smoothed_xi_gn_p_results = gaussian_filter(xi_gn_p_results, sigma= sigma )\n",
    "smoothed_xi_gn_x_results = gaussian_filter(xi_gn_x_results, sigma= sigma )\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(smoothed_xi_gn_p_results, extent=[0, 200, -200, 200])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(smoothed_xi_gn_x_results, extent=[0, 200, -200, 200])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists to store results\n",
    "# sticks_xi_gn_p_results = []\n",
    "# sticks_xi_gn_x_results = []\n",
    "# r_results = []\n",
    "\n",
    "# # Iterate over rpar bins\n",
    "# for i in range(len(rpar_bins) - 1):\n",
    "\n",
    "#     min_rpar = rpar_bins[i]\n",
    "#     max_rpar = rpar_bins[i + 1]\n",
    "\n",
    "#     r , xi_p , xi_x = process_ng_rpar_bin( stick_catalogue , min_rpar, max_rpar , min_sep , max_sep , nbins )\n",
    "\n",
    "#     # Store the results\n",
    "#     sticks_xi_gn_p_results.append( xi_p )\n",
    "#     sticks_xi_gn_x_results.append( xi_x )\n",
    "#     r_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace( 0.5 , 200 , 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming r_perp_bins and r_par_bins are defined\n",
    "r_perp_cents = r = np.linspace( 0.5 , 200 , 100 )\n",
    "r_par_cents = ( rpar_bins[1:] + rpar_bins[:-1] )/2.\n",
    "\n",
    "# Flatten the xi_results and r_results arrays\n",
    "xi_gn_p_flat = np.array(xi_gn_p_results).flatten()\n",
    "xi_gn_x_flat = np.array(xi_gn_x_results).flatten()\n",
    "\n",
    "# sticks_xi_gn_p_flat = np.array(sticks_xi_gn_p_results).flatten()\n",
    "# sticks_xi_gn_x_flat = np.array(sticks_xi_gn_x_results).flatten()\n",
    "\n",
    "\n",
    "r_perp_flat = np.array([r_perp_cents] * len(r_par_cents)).flatten()\n",
    "r_par_flat = np.array([[r_par] * len(r_perp_cents) for r_par in r_par_cents]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate s for each pair of (r_perp, r_par)\n",
    "s = np.sqrt(r_perp_flat**2 + r_par_flat**2)\n",
    "\n",
    "# Define bins for s\n",
    "s_bins = np.linspace(0.5, 200, 30 )\n",
    "s_bins = np.logspace( np.log10( 0.1 ) , np.log10( 200 ) , 45 )\n",
    "s_cents = (s_bins[1:] + s_bins[:-1]) / 2.\n",
    "\n",
    "# Initialize arrays to store the results\n",
    "xi_p_s = np.zeros(len(s_bins) - 1)\n",
    "var_xi_p_s = np.zeros(len(s_bins) - 1)\n",
    "counts_p = np.zeros(len(s_bins) - 1)\n",
    "\n",
    "xi_x_s = np.zeros(len(s_bins) - 1)\n",
    "var_xi_x_s = np.zeros(len(s_bins) - 1)\n",
    "counts_x = np.zeros(len(s_bins) - 1)\n",
    "\n",
    "# Calculate theta\n",
    "theta = np.degrees(np.arctan2(r_par_flat, r_perp_flat))\n",
    "\n",
    "# Bin the results by s and calculate the radial averages and variances\n",
    "for i in range(len(s_bins) - 1):\n",
    "    mask = (s >= s_bins[i]) & (s < s_bins[i + 1])\n",
    "    if np.any(mask):  # Check if there are any elements in the mask\n",
    "        xi_p_s[i] = np.mean(xi_gn_p_flat[mask])\n",
    "        var_xi_p_s[i] = np.var(xi_gn_p_flat[mask])\n",
    "        counts_p[i] = np.sum(mask)\n",
    "\n",
    "        xi_x_s[i] = np.mean(xi_gn_x_flat[mask])\n",
    "        var_xi_x_s[i] = np.var(xi_gn_x_flat[mask])\n",
    "        counts_x[i] = np.sum(mask)\n",
    "\n",
    "# Handle cases where there are no counts in a bin\n",
    "xi_p_s[counts_p == 0] = np.nan\n",
    "var_xi_p_s[counts_p == 0] = np.nan\n",
    "\n",
    "xi_x_s[counts_x == 0] = np.nan\n",
    "var_xi_x_s[counts_x == 0] = np.nan\n",
    "\n",
    "# Calculate the average within the specified theta ranges and r_perp >= 5\n",
    "theta_mask = (((theta > -45) & (theta < 45)) | ((theta > 135) & (theta < 225))) & (r_perp_flat >= 5)\n",
    "\n",
    "xi_p_s_theta = np.zeros(len(s_bins) - 1)\n",
    "var_xi_p_s_theta = np.zeros(len(s_bins) - 1)\n",
    "counts_p_theta = np.zeros(len(s_bins) - 1)\n",
    "\n",
    "xi_x_s_theta = np.zeros(len(s_bins) - 1)\n",
    "var_xi_x_s_theta = np.zeros(len(s_bins) - 1)\n",
    "counts_x_theta = np.zeros(len(s_bins) - 1)\n",
    "\n",
    "for i in range(len(s_bins) - 1):\n",
    "    mask = (s >= s_bins[i]) & (s < s_bins[i + 1]) & theta_mask \n",
    "    if np.any(mask):  # Check if there are any elements in the mask\n",
    "        xi_p_s_theta[i] = np.mean(xi_gn_p_flat[mask])\n",
    "        var_xi_p_s_theta[i] = np.var(xi_gn_p_flat[mask])\n",
    "        counts_p_theta[i] = np.sum(mask)\n",
    "\n",
    "        xi_x_s_theta[i] = np.mean(xi_gn_x_flat[mask])\n",
    "        var_xi_x_s_theta[i] = np.var(xi_gn_x_flat[mask])\n",
    "        counts_x_theta[i] = np.sum(mask)\n",
    "\n",
    "# Handle cases where there are no counts in a bin\n",
    "xi_p_s_theta[counts_p_theta == 0] = np.nan\n",
    "var_xi_p_s_theta[counts_p_theta == 0] = np.nan\n",
    "\n",
    "xi_x_s_theta[counts_x_theta == 0] = np.nan\n",
    "var_xi_x_s_theta[counts_x_theta == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h =0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.errorbar( s_cents * h , xi_p_s * s_cents**2, fmt='o' , yerr = var_xi_p_s**0.5  * s_cents**2 / counts_x**0.5 )\n",
    "plt.errorbar( s_cents * h , -xi_p_s_theta, fmt='ko' , yerr = var_xi_p_s**0.5  / counts_x**0.5  )\n",
    "#plt.plot(s_cents * h, np.zeros(s_cents.shape), 'k--' )\n",
    "plt.axvline(x=150 * h, color='r', linestyle='--', label='BAO scale')\n",
    "\n",
    "# plt.errorbar( s_cents , -xi_x_s * s_cents**2, fmt='o' , yerr = var_xi_x_s**0.5  * s_cents**2 / counts_p**0.5 )\n",
    "#plt.errorbar( s_cents * h , -sticks_xi_p_s * s_cents**2, fmt='o' , yerr = var_xi_p_s**0.5  * s_cents**2 / counts_x**0.5 )\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$ \\xi_{g+}(s)$')\n",
    "#plt.xlim( 50 , 150 )\n",
    "#plt.ylim( -12 , 6 )\n",
    "#plt.ylim( -10 , 20 )\n",
    "#plt.ylim( -0.01 , 0.03 * 100**2 )\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.errorbar( s_cents * h , xi_p_s * s_cents**2, fmt='o' , yerr = var_xi_p_s**0.5  * s_cents**2 / counts_x**0.5 )\n",
    "plt.errorbar( s_cents * h , xi_p_s_theta * s_cents**2, fmt='ko' , yerr = var_xi_p_s**0.5  * s_cents**2 / counts_x**0.5  )\n",
    "#plt.plot(s_cents * h, np.zeros(s_cents.shape), 'k--' )\n",
    "plt.axvline(x=150 * h, color='r', linestyle='--', label='BAO scale')\n",
    "\n",
    "# plt.errorbar( s_cents , -xi_x_s * s_cents**2, fmt='o' , yerr = var_xi_x_s**0.5  * s_cents**2 / counts_p**0.5 )\n",
    "#plt.errorbar( s_cents * h , -sticks_xi_p_s * s_cents**2, fmt='o' , yerr = var_xi_p_s**0.5  * s_cents**2 / counts_x**0.5 )\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$s^2 \\xi_{g+}(s)$')\n",
    "plt.xlim( 50 , 150 )\n",
    "plt.ylim( -12 , 6 )\n",
    "#plt.ylim( -10 , 20 )\n",
    "#plt.ylim( -0.01 , 0.03 * 100**2 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar( s_cents * h , xi_p_s * s_cents**2, fmt='ko' , yerr = var_xi_p_s**0.5  * s_cents**2 / counts_x**0.5 )\n",
    "#plt.plot(s_cents * h, np.zeros(s_cents.shape), 'k--' )\n",
    "plt.axvline(x=150 * h, color='r', linestyle='--', label='BAO scale')\n",
    "\n",
    "# plt.errorbar( s_cents , -xi_x_s * s_cents**2, fmt='o' , yerr = var_xi_x_s**0.5  * s_cents**2 / counts_p**0.5 )\n",
    "#plt.errorbar( s_cents * h , -sticks_xi_p_s * s_cents**2, fmt='o' , yerr = var_xi_p_s**0.5  * s_cents**2 / counts_x**0.5 )\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Xi(s)')\n",
    "plt.title('Radial Averages of Xi(s)')\n",
    "plt.xlim( 50 , 150 )\n",
    "plt.ylim( -12 , 6 )\n",
    "#plt.ylim( -10 , 20 )\n",
    "#plt.ylim( -0.01 , 0.03 * 100**2 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(s_cents * h, -xi_p_s , fmt='o', yerr=var_xi_p_s**0.5 / counts_x**0.5 )\n",
    "# plt.errorbar(s_cents, xi_s, fmt='o', yerr=var_xi_s**0.5 / counts**0.5, label='Xi_s(s)')\n",
    "# plt.errorbar(s_cents, -xi_x_s, fmt='o', yerr=var_xi_x_s**0.5 / counts_p**0.5, label='Xi_x(s)')\n",
    "plt.plot(s_cents * h, np.zeros(s_cents.shape), 'k--' )\n",
    "plt.axvline(x=150 * h, color='r', linestyle='--', label='BAO scale')\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$\\xi_{g+}(s)$')\n",
    "#plt.xlim( 50 , 150 )\n",
    "#plt.ylim( -0.002, 0.0008 )\n",
    "plt.yscale('log')\n",
    "#plt.ylim( -0.001, 0.0008 )\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(s_cents * h, xi_p_s, fmt='o', yerr=var_xi_p_s**0.5 / counts_x**0.5 )\n",
    "# plt.errorbar(s_cents, xi_s, fmt='o', yerr=var_xi_s**0.5 / counts**0.5, label='Xi_s(s)')\n",
    "# plt.errorbar(s_cents, -xi_x_s, fmt='o', yerr=var_xi_x_s**0.5 / counts_p**0.5, label='Xi_x(s)')\n",
    "plt.plot(s_cents * h, np.zeros(s_cents.shape), 'k--' )\n",
    "plt.axvline(x=150 * h, color='r', linestyle='--', label='BAO scale')\n",
    "plt.xlabel('s')\n",
    "plt.ylabel(r'$\\xi_{g+}(s)$')\n",
    "plt.xlim( 50 , 150 )\n",
    "plt.ylim( -0.0004, 0.0002 )\n",
    "# plt.yscale('log')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shear stuff\n",
    "# Function to process a single rpar bin\n",
    "def process_gg_rpar_bin( shape_catalogue , min_rpar, max_rpar , min_sep = min_sep, max_sep = max_sep, nbins = nbins):\n",
    "    print('Running between rpar =', min_rpar, 'and rpar =', max_rpar)\n",
    "\n",
    "    # Create the NNCorrelation objects\n",
    "    gg = treecorr.GGCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar, bin_type='Linear') \n",
    "\n",
    "    # Process the position and random catalogues\n",
    "    gg.process(shape_catalogue, shape_catalogue, metric='Rperp')\n",
    "\n",
    "    xip = gg.xip\n",
    "    xim = gg.xim\n",
    "    r = np.exp(gg.meanlogr)\n",
    "\n",
    "    return r , xip , xim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range and number of bins for rpar\n",
    "rpar_bins = np.linspace(-200, 200, 201 )  \n",
    "\n",
    "# Define the parameters for the correlation function\n",
    "min_sep = 0.5  # Minimum separation in Mpc\n",
    "max_sep = 200  # Maximum separation in Mpc\n",
    "nbins = 100     # Number of bins\n",
    "bin_slop = 0.1 # Bin slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "xi_gg_p_results = []\n",
    "xi_gg_m_results = []\n",
    "r_results = []\n",
    "\n",
    "# Iterate over rpar bins\n",
    "for i in range(len(rpar_bins) - 1):\n",
    "\n",
    "    min_rpar = rpar_bins[i]\n",
    "    max_rpar = rpar_bins[i + 1]\n",
    "\n",
    "    r , xi_p , xi_m = process_gg_rpar_bin( shape_catalogue , min_rpar, max_rpar , min_sep , max_sep , nbins )\n",
    "\n",
    "    # Store the results\n",
    "    xi_gg_p_results.append( xi_p )\n",
    "    xi_gg_m_results.append( xi_m )\n",
    "    r_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10 , 5 ))\n",
    "plt.subplot(121)\n",
    "plt.imshow( ( xi_gg_p_results ) )\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow( ( xi_gg_m_results ) )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sigma = 2\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "smoothed_xi_gg_p_results = gaussian_filter(xi_gg_p_results, sigma= sigma )\n",
    "smoothed_xi_gg_m_results = gaussian_filter(xi_gg_m_results, sigma= sigma )\n",
    "\n",
    "image_lim = 100\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(smoothed_xi_gg_p_results, extent=[0, 200, -200, 200])\n",
    "plt.xlim(  0 ,image_lim )\n",
    "plt.ylim( -image_lim ,image_lim )\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(smoothed_xi_gg_m_results, extent=[0, 200, -200, 200])\n",
    "plt.xlim(  0 ,image_lim )\n",
    "plt.ylim( -image_lim ,image_lim )\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the xi_gg_p_results and xi_gg_m_results arrays\n",
    "xi_gg_p_flat = np.array(xi_gg_p_results).flatten()\n",
    "xi_gg_m_flat = np.array(xi_gg_m_results).flatten()\n",
    "\n",
    "# Calculate s for each pair of (r_perp, r_par)\n",
    "s = np.sqrt(r_perp_flat**2 + r_par_flat**2)\n",
    "\n",
    "# Define bins for s\n",
    "s_bins = np.linspace(5, 250, 20)\n",
    "s_cents = (s_bins[1:] + s_bins[:-1]) / 2.\n",
    "\n",
    "# Initialize arrays to store the results\n",
    "xi_gg_p_s = np.zeros(len(s_bins) - 1)\n",
    "var_xi_gg_p_s = np.zeros(len(s_bins) - 1)\n",
    "counts_p = np.zeros(len(s_bins) - 1)\n",
    "\n",
    "xi_gg_m_s = np.zeros(len(s_bins) - 1)\n",
    "var_xi_gg_m_s = np.zeros(len(s_bins) - 1)\n",
    "counts_m = np.zeros(len(s_bins) - 1)\n",
    "\n",
    "# Bin the results by s and calculate the radial averages and variances\n",
    "for i in range(len(s_bins) - 1):\n",
    "    mask = (s >= s_bins[i]) & (s < s_bins[i + 1])\n",
    "    if np.any(mask):  # Check if there are any elements in the mask\n",
    "        xi_gg_p_s[i] = np.mean(xi_gg_p_flat[mask])\n",
    "        var_xi_gg_p_s[i] = np.var(xi_gg_p_flat[mask])\n",
    "        counts_p[i] = np.sum(mask)\n",
    "\n",
    "        xi_gg_m_s[i] = np.mean(xi_gg_m_flat[mask])\n",
    "        var_xi_gg_m_s[i] = np.var(xi_gg_m_flat[mask])\n",
    "        counts_m[i] = np.sum(mask)\n",
    "\n",
    "# Handle cases where there are no counts in a bin\n",
    "xi_gg_p_s[counts_p == 0] = np.nan\n",
    "var_xi_gg_p_s[counts_p == 0] = np.nan\n",
    "\n",
    "xi_gg_m_s[counts_m == 0] = np.nan\n",
    "var_xi_gg_m_s[counts_m == 0] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar( s_cents * h , xi_gg_m_s, fmt='o' , yerr = var_xi_gg_m_s**0.5 / counts_m**0.5 )\n",
    "# plt.errorbar( s_cents , xi_s , fmt='o' , yerr = var_xi_s**0.5 / counts**0.5 )\n",
    "# plt.errorbar( s_cents * h , xi_gg_p_s, fmt='o' , yerr = var_xi_gg_p_s**0.5 / counts_p**0.5 )\n",
    "\n",
    "#plt.errorbar( s_cents , -xi_x_s, fmt='o' , yerr = var_xi_x_s**0.5 / counts_p**0.5 )\n",
    "plt.plot( s_cents *h , np.zeros( s_cents.shape ) , 'k--')\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Xi(s)')\n",
    "plt.title('Radial Averages of Xi(s)')\n",
    "#plt.xlim( 5 * h , 250 * h )\n",
    "plt.ylim( -0.0001 , 0.0001 ) \n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar( s_cents * h , xi_gg_p_s, fmt='o' , yerr = var_xi_gg_m_s**0.5 / counts_m**0.5 )\n",
    "# plt.errorbar( s_cents , xi_s , fmt='o' , yerr = var_xi_s**0.5 / counts**0.5 )\n",
    "# plt.errorbar( s_cents * h , xi_gg_p_s, fmt='o' , yerr = var_xi_gg_p_s**0.5 / counts_p**0.5 )\n",
    "\n",
    "#plt.errorbar( s_cents , -xi_x_s, fmt='o' , yerr = var_xi_x_s**0.5 / counts_p**0.5 )\n",
    "plt.plot( s_cents *h , np.zeros( s_cents.shape ) , 'k--')\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Xi(s)')\n",
    "plt.title('Radial Averages of Xi(s)')\n",
    "#plt.xlim( 5 * h , 250 * h )\n",
    "plt.ylim( -0.0001 , 0.0001 ) \n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the NNCorrelation objects\n",
    "ng = treecorr.NGCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar) \n",
    "rg = treecorr.NGCorrelation(min_sep=min_sep, max_sep=max_sep, nbins=nbins, min_rpar=min_rpar, max_rpar=max_rpar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the position and random catalogues\n",
    "ng.process( position_catalogue , shape_catalogue , metric ='Rperp' )\n",
    "rg.process( random_position_catalogue , shape_catalogue, metric ='Rperp' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_p = ng.raw_xi\n",
    "xim_p = ng.xi_im "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation function\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(r, xi_p, label=r'$\\Xi(s)$')\n",
    "plt.plot(r, xim_p, label=r'$\\Xi(s)$')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Separation (Mpc)')\n",
    "plt.ylabel(r'$\\Xi(s)$')\n",
    "plt.title('Landy-Szalay Correlation Function $\\Xi(s)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_corr import compute_2p_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters related with bins\n",
    "nbins = 10 # number of radial bins\n",
    "rmin = 0.1 # minimum value for rp (r in case of the quadrupole)\n",
    "rmax = 200. # maximum value for rp (r in case of the quadrupole)\n",
    "pi_max = 15. # maximum value along l.o.s. (Pi) \n",
    "npi = 5 # number of bins in Pi\n",
    "mubins = 10 # number of bins in mu\n",
    "grid_resolution = 10 # Controls the resolution of the r,mu grid to compute the quadrupole, \n",
    "#you can lower this value for a faster computation but it's going to compute wgp2 only at larger radius\n",
    "\n",
    "# Related to JK patches\n",
    "NPatches = 15 #int(nbins**(3./2.))\n",
    "print('Number of patches',NPatches)\n",
    "# Other configuration parameters\n",
    "ncores = 30 # Number of cores to run in parallel\n",
    "slop = 0.1 # Resolution for treecorr\n",
    "box = False # Indicates if the data corresponds to a box, otherwise it will assume a lightcone\n",
    "exact_position = False # Indicates if the coordinates are exactly provided (e.g. simulated data \n",
    "# without any error added in the position). Otherwise it will assume that the positions are not\n",
    "# exact and will use ra, dec to match the catalogues. If this parameter is set as True, it\n",
    "# will neglect box and it will set it as False\n",
    "sky_threshold = 1.0 # Threshold for matching the catalogues in arcsecond, used if exact_position\n",
    "#is set to True.\n",
    "grid_resolution = 10 # Resolution of the grid used to compute the quadrupole, a lower number will\n",
    "#speed up the computation but reaches to larger radial distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['RA','DEC','d','e1','e2', 'w']\n",
    "\n",
    "config = {'col_names': col_names,\n",
    "          'nbins': nbins,\n",
    "          'rmin': rmin,\n",
    "          'rmax': rmax,\n",
    "          'pi_max': pi_max,\n",
    "          'npi': npi,\n",
    "          'grid_resolution': grid_resolution,\n",
    "          'mubins': mubins,\n",
    "          'NPatches': NPatches,\n",
    "          'ncores': ncores,\n",
    "          'slop': slop,\n",
    "          'box': box,\n",
    "          'exact_position': exact_position,\n",
    "          'sky_threshold': sky_threshold,\n",
    "          'grid_resolution': grid_resolution\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = compute_2p_corr(positions,shapes,random_positions,random_shapes,config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.compute_wgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(corr.wgg.rp,corr.wgg.xip,yerr=corr.wgg.std_from_cov,label=r'$w_{gg}[Mpc/h]$',c='lightcoral')\n",
    "# plt.errorbar(corr.wgp.rp,corr.wgp.xip,yerr=corr.wgp.std_from_cov,label=r'$w_{g+}[Mpc/h]$',c='C3')\n",
    "# plt.errorbar(corr.wgp2.r,corr.wgp2.xip,yerr=corr.wgp2.std_from_cov,label=r'$w_{g+,2}[Mpc/h]$',c='brown')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'$w[Mpc/h]$',fontsize=14)\n",
    "plt.xlabel(r'$r(_p) [Mpc/h]$',fontsize=14)\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.compute_wgp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.errorbar(corr.wgg.rp,\n",
    "#              corr.wgg.xip,yerr=corr.wgg.std_from_cov,label=r'$w_{gg}[Mpc/h]$',c='lightcoral')\n",
    "plt.errorbar( corr.wgp.rp,\n",
    "              -corr.wgp.xip,\n",
    "              yerr=corr.wgp.std_from_cov,label=r'$w_{g+}[Mpc/h]$',c='C3')\n",
    "plt.errorbar( corr.wgp.rp,\n",
    "              corr.wgx.xip,\n",
    "              yerr=corr.wgx.std_from_cov,label=r'$w_{gx}[Mpc/h]$',c='b')\n",
    "# plt.errorbar(corr.wgp.rp,-corr.wgp.xip,yerr=corr.wgp.std_from_cov,label=r'$w_{g+}[Mpc/h]$',c='C3')\n",
    "# plt.errorbar(corr.wgp2.r,corr.wgp2.xip,yerr=corr.wgp2.std_from_cov,label=r'$w_{g+,2}[Mpc/h]$',c='brown')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'$w[Mpc/h]$',fontsize=14)\n",
    "plt.xlabel(r'$r(_p) [Mpc/h]$',fontsize=14)\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(corr.wgp.rp,corr.wgx.xip,yerr=corr.wgp.std_from_cov,label=r'$w_{gx}[Mpc/h]$',c='C0')\n",
    "plt.errorbar( corr.wgp.rp,\n",
    "              -corr.wgp.xip,\n",
    "              yerr=corr.wgp.std_from_cov,label=r'$w_{g+}[Mpc/h]$',c='C3')\n",
    "plt.plot(corr.wgp.rp,corr.wgp.rp*0.,'C7--')\n",
    "plt.xscale('log')\n",
    "plt.ylabel(r'$w_\\times[Mpc/h]$',fontsize=14)\n",
    "plt.xlabel(r'$r(_p) [Mpc/h]$',fontsize=14)\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for TreeCorr\n",
    "# shape_catalogue = treecorr.Catalog( ra=matched_shape_ra, \n",
    "#                                     dec=matched_shape_dec,\n",
    "#                                     g1=matched_shape_e1, \n",
    "#                                     g2=matched_shape_e2 , \n",
    "#                                     w = matched_shape_w , \n",
    "#                                     r = r_shapes ,\n",
    "#                                     ra_units='deg', dec_units='deg')\n",
    "\n",
    "\n",
    "# position_catalogue = treecorr.Catalog( ra = galaxy_sample['RA'],\n",
    "#                                        dec = galaxy_sample['DEC'] , \n",
    "#                                        r = r_positions , \n",
    "#                                        w = galaxy_sample['WEIGHT_ALL_NOFKP'] , \n",
    "#                                        ra_units='deg', \n",
    "#                                        dec_units='deg' )\n",
    "\n",
    "shape_catalogue = treecorr.Catalog( ra=matched_shape_ra, \n",
    "                                    dec=matched_shape_dec,\n",
    "                                    g1=matched_shape_e1, \n",
    "                                    g2=matched_shape_e2 , \n",
    "                                    w = matched_shape_w , \n",
    "                                    #r = r_shapes ,\n",
    "                                    ra_units='deg', dec_units='deg')\n",
    "\n",
    "\n",
    "position_catalogue = treecorr.Catalog( ra = galaxy_sample['RA'],\n",
    "                                       dec = galaxy_sample['DEC'] , \n",
    "                                       #r = r_positions , \n",
    "                                       w = galaxy_sample['WEIGHT_ALL_NOFKP'] , \n",
    "                                       ra_units='deg', \n",
    "                                       dec_units='deg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rpar = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gg = treecorr.GGCorrelation( nbins=20, min_sep=0.1, max_sep=100, max_rpar=max_rpar)\n",
    "gg = treecorr.GGCorrelation( nbins=20, min_sep=0.1, max_sep=100,sep_units='arcmin' )\n",
    "\n",
    "gg.process( shape_catalogue )\n",
    "\n",
    "xip = gg.xip  # The xi_plus correlation function\n",
    "xim = gg.xim  # The xi_minus correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_redshift_cut = ( galaxy_sample['Z'] > 0.6 )  &  ( galaxy_sample['Z'] < 0.65 )\n",
    "shape_redshift_cut = ( matched_galaxy_redshift > 0.6 )  &  ( matched_galaxy_redshift < 0.65 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( matched_shape_ra[shape_redshift_cut] ), len( galaxy_sample['RA'][galaxy_redshift_cut] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_catalogue = treecorr.Catalog( ra=matched_shape_ra[shape_redshift_cut], \n",
    "                                    dec=matched_shape_dec[shape_redshift_cut],\n",
    "                                    g1=matched_shape_e1[shape_redshift_cut], \n",
    "                                    g2=matched_shape_e2[shape_redshift_cut] , \n",
    "                                    w = matched_shape_w[shape_redshift_cut] , \n",
    "                                    #r = r_shapes ,\n",
    "                                    ra_units='deg', dec_units='deg')\n",
    "\n",
    "\n",
    "position_catalogue = treecorr.Catalog( ra = galaxy_sample['RA'][galaxy_redshift_cut],\n",
    "                                       dec = galaxy_sample['DEC'][galaxy_redshift_cut] , \n",
    "                                       #r = r_positions , \n",
    "                                       w = galaxy_sample['WEIGHT_ALL_NOFKP'][galaxy_redshift_cut] , \n",
    "                                       ra_units='deg', \n",
    "                                       dec_units='deg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = treecorr.NNCorrelation( min_sep=.1, max_sep=60., bin_size=0.1, sep_units='degrees'  )\n",
    "\n",
    "nn.process( position_catalogue , position_catalogue )\n",
    "print(nn.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = treecorr.Catalog(ra=random_positions[ra], dec=random_positions[dec], ra_units='degrees', dec_units='degrees')\n",
    "rr = treecorr.NNCorrelation( min_sep=.1, max_sep=60., bin_size=0.1, sep_units='degrees')\n",
    "rr.process(rand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = treecorr.NNCorrelation( min_sep=.1, max_sep=60., bin_size=0.1, sep_units='degrees' )\n",
    "dr.process( position_catalogue , rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xi, varxi = nn.calculateXi(rr=rr, dr=dr)\n",
    "sig = np.sqrt(varxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = np.exp(nn.meanlogr)\n",
    "sig = np.sqrt(varxi)\n",
    "\n",
    "plt.plot(r, xi, color='blue')\n",
    "plt.plot(r, -xi, color='blue', ls=':')\n",
    "plt.errorbar(r[xi>0], xi[xi>0], yerr=sig[xi>0], color='blue', lw=0.1, ls='')\n",
    "plt.errorbar(r[xi<0], -xi[xi<0], yerr=sig[xi<0], color='blue', lw=0.1, ls='')\n",
    "leg = plt.errorbar(-r, xi, yerr=sig, color='blue')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log', nonpositive='clip')\n",
    "plt.xlabel(r'$\\theta$ (degrees)')\n",
    "\n",
    "plt.legend([leg], [r'$w(\\theta)$'], loc='lower left')\n",
    "# plt.xlim([0.01,10])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = treecorr.NGCorrelation( min_sep=.01, max_sep=60., bin_size=0.4, sep_units='degrees' )\n",
    "\n",
    "\n",
    "ng.process( position_catalogue , shape_catalogue )\n",
    "\n",
    "ng_xip = ng.xi  # The xi_plus correlation function\n",
    "ng_xim = ng.xi_im  # The xi_minus correlation function\n",
    "ngr = np.exp( ng.meanlogr )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( ngr , -ng_xip )\n",
    "plt.plot( ngr ,  ng_xim )\n",
    "plt.plot( ngr , ng_xip , '--' )\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Separation (Mpc/h)')\n",
    "plt.ylabel('xi_plus')\n",
    "plt.title('Shear-Shear Correlation Function (xi_plus)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# Define redshift bins\n",
    "#redshift_bins = [(0.6, 0.65), (0.65, 0.7), (0.7, 0.75), (0.75, 0.8), (0.8, 0.85), (0.9, 0.95), (0.95, 1)]\n",
    "dz = 0.03\n",
    "redshift_bins = [(z, z + dz) for z in np.arange(0.6, 1, dz)]\n",
    "\n",
    "# Loop over redshift bins\n",
    "for z_min, z_max in redshift_bins:\n",
    "    galaxy_redshift_cut = (galaxy_sample['Z'] > z_min) & (galaxy_sample['Z'] < z_max)\n",
    "    shape_redshift_cut = (matched_galaxy_redshift > z_min) & (matched_galaxy_redshift < z_max)\n",
    "\n",
    "    rgalaxy_redshift_cut = (random_positions[redshift] > z_min) & ( random_positions[redshift]  < z_max)\n",
    "    rshape_redshift_cut = (random_shapes[redshift] > z_min) & ( random_shapes[redshift]  < z_max)\n",
    "\n",
    "    shape_catalogue = treecorr.Catalog(\n",
    "        ra=matched_shape_ra[shape_redshift_cut],\n",
    "        dec=matched_shape_dec[shape_redshift_cut],\n",
    "        g1=matched_shape_e1[shape_redshift_cut],\n",
    "        g2=matched_shape_e2[shape_redshift_cut],\n",
    "        w=matched_shape_w[shape_redshift_cut],\n",
    "        ra_units='deg',\n",
    "        dec_units='deg'\n",
    "    )\n",
    "\n",
    "    position_catalogue = treecorr.Catalog(\n",
    "        ra=galaxy_sample['RA'][galaxy_redshift_cut],\n",
    "        dec=galaxy_sample['DEC'][galaxy_redshift_cut],\n",
    "        w=galaxy_sample['WEIGHT_ALL_NOFKP'][galaxy_redshift_cut],\n",
    "        ra_units='deg',\n",
    "        dec_units='deg'\n",
    "    )\n",
    "\n",
    "    random_position_catalogue = treecorr.Catalog(\n",
    "        ra=random_positions[ra][rgalaxy_redshift_cut],\n",
    "        dec=random_positions[dec][rgalaxy_redshift_cut],\n",
    "        ra_units='deg',\n",
    "        dec_units='deg'\n",
    "    )\n",
    "\n",
    "    random_shape_catalogue = treecorr.Catalog(\n",
    "        ra=random_shapes[ra][rshape_redshift_cut],\n",
    "        dec=random_shapes[dec][rshape_redshift_cut],\n",
    "        ra_units='deg',\n",
    "        dec_units='deg'\n",
    "    )\n",
    "\n",
    "    ng = treecorr.NGCorrelation(min_sep=.01, max_sep=600., bin_size=0.35, sep_units='degrees')\n",
    "    rg = treecorr.NGCorrelation(min_sep=.01, max_sep=600., bin_size=0.35, sep_units='degrees')\n",
    "    #rr = treecorr.NNCorrelation(min_sep=.001, max_sep=600., bin_size=0.15, sep_units='degrees')\n",
    "\n",
    "    ng.process(position_catalogue, shape_catalogue)\n",
    "    rg.process(random_position_catalogue, shape_catalogue)\n",
    "    #rr.process(random_position_catalogue, random_shape_catalogue)\n",
    "\n",
    "    ng_xip =  ng.xi - rg.xi\n",
    "    ng_xim = ng.xi_im - rg.xi_im  # The xi_minus correlation function\n",
    "    varxi = ng.varxi  # The variance of the correlation function\n",
    "    ngr = np.exp(ng.meanlogr)\n",
    "\n",
    "    print( len( ngr ), len( ng_xip ), len( ng_xim ), len( varxi ) )\n",
    "\n",
    "    data.append( [ngr, ng_xip, ng_xim, varxi] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:, 3][ data[:, 3] == 0] = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted mean\n",
    "weighted_mean = np.average(data[:, 1], axis=0, weights=1/data[:, 3])\n",
    "weighted_mean_im = np.average(data[:, 2], axis=0, weights=1/data[:, 3])\n",
    "\n",
    "# Calculate the weighted variance\n",
    "weighted_variance = np.average(data[:, 3], axis=0, weights=1/data[:, 3])\n",
    "\n",
    "# Calculate the weighted standard deviation (error)\n",
    "weighted_err = (weighted_variance)**0.5\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(ngr, -weighted_mean, yerr=weighted_err, fmt='--')\n",
    "plt.errorbar(ngr, -weighted_mean_im, yerr=weighted_err, fmt='--')\n",
    "#plt.errorbar(ngr, weighted_mean, yerr=weighted_err, fmt='--')\n",
    "plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('Separation (degrees)')\n",
    "plt.ylabel('xi_plus')\n",
    "plt.title('Weighted Mean and Error of xi_plus')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
